{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medellin Reales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_B_M = pd.read_csv('datos_sinnorm_sinimput.csv') \n",
    "df_B_M = df_B_M.loc[:, ~df_B_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#Medellin\n",
    "\n",
    "df_M = df_B_M.loc[df_B_M['medellin'] == 1]\n",
    "df_M = df_M.dropna(subset = ['lat', 'lon'])\n",
    "df_m = df_M[['lon', 'lat']]\n",
    "\n",
    "#Eliminacion de Datos Outliers\n",
    "\n",
    "df_m = df_m.loc[df_m['lat'] >= 6.1]\n",
    "df_m = df_m.loc[df_m['lat'] <= 6.4]\n",
    "df_m = df_m.loc[df_m['lon'] >= -75.71034908]\n",
    "df_m = df_m.loc[df_m['lon'] <= -75.4]\n",
    "\n",
    "#Creacion del GeoDataFrame con los datos restantes\n",
    "\n",
    "gdf_geom = gpd.GeoDataFrame(df_m.copy(), geometry = gpd.points_from_xy(df_m.lon, df_m.lat), crs={'init': 'epsg:4326'})\n",
    "#gdf_geom.to_file('C:/Users/danie/Documents/Daniel/SIMAT/Ubicaciones_Medellin/Ubicaciones_Medellin_Datos_Reales.shp')\n",
    "\n",
    "#Cargar el mapa de Medellin\n",
    "\n",
    "sectores = gpd.read_file('C:/Users/danie/Documents/Daniel/SIMAT/Límite_Catastral_de__Comunas_y_Corregimientos/Límite_Catastral_de__Comunas_y_Corregimientos.shp')\n",
    "#puntos_de_venta_medellin_reales = gpd.read_file('C:/Users/danie/Documents/Daniel/SIMAT/Ubicaciones_Medellin/Ubicaciones_Medellin_Datos_Reales.shp')\n",
    "\n",
    "#Graficar los puntos en el mapa\n",
    "\n",
    "#sectores.plot(cmap = 'jet', column = 'NOMBRE', figsize = (10,10))\n",
    "#puntos_de_venta_medellin_reales.plot()\n",
    "ax = sectores.plot(color='white', edgecolor='black')\n",
    "gdf_geom.plot(ax = ax, color = 'red')\n",
    "\n",
    "#Crear un nuevo GeodataFrame que unicamente contiene los datos que se encuentran dentro del mapa\n",
    "\n",
    "sectores_con_puntos_de_venta = gpd.sjoin(gdf_geom, sectores, how = \"left\", op = 'intersects')\n",
    "\n",
    "#Filtrar el GeoDataFrame, eliminando los datos que no se encuentran en el mapa\n",
    "\n",
    "sectores_con_puntos_de_venta_Medellin = sectores_con_puntos_de_venta.loc[sectores_con_puntos_de_venta.isna()['SECTOR'].apply(lambda value: not value)]\n",
    "\n",
    "#Graficar el todos los puntos que se encuentran dentro del mapa\n",
    "\n",
    "ax = sectores.plot(color='white', edgecolor='black')\n",
    "sectores_con_puntos_de_venta_Medellin.plot(ax = ax, color = 'red')\n",
    "\n",
    "#Agregar una columna al DataSet inicial que indique cuales son los puntos que realmente se encuentran dentro del mapa\n",
    "\n",
    "Medellin_Reales = np.zeros(df_B_M.shape[0])\n",
    "for i in sectores_con_puntos_de_venta_Medellin.index:\n",
    "    Medellin_Reales[i] = 1\n",
    "df_B_M['Medellin Reales'] = Medellin_Reales\n",
    "\n",
    "#Crear nuevo dataset\n",
    "#df_B_M.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_sinnorm_sinimput_con_Medellin_reales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sectores.plot(cmap = 'jet', column = 'NOMBRE', figsize = (10,10))\n",
    "#puntos_de_venta_medellin_reales.plot()\n",
    "ax = sectores.plot(color='white', edgecolor='black')\n",
    "gdf_geom.plot(ax = ax, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sectores.plot(color='white', edgecolor='black')\n",
    "sectores_con_puntos_de_venta_Medellin.plot(ax = ax, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos solamente Medellin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from random import uniform\n",
    "\n",
    "df_B_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_sinnorm_sinimput_con_Medellin_reales.csv')\n",
    "df_B_M = df_B_M.loc[:, ~df_B_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#filtrar datos que no son de Medellin y unirlos a los que necesitan input de coordenadas\n",
    "df_M = df_B_M.loc[df_B_M['medellin'] == 1]\n",
    "df_M_imput = df_M.loc[df_M['lat'].isna()].copy()\n",
    "df_M_reales = df_M.loc[df_M['Medellin Reales'] == 1]\n",
    "\n",
    "#Imputacion de coordenadas\n",
    "df_M_imput['lon'] = df_M_imput['lon'].apply(lambda x: uniform(-75.625,-75.54))\n",
    "df_M_imput['lat'] = df_M_imput['lat'].apply(lambda x: uniform(6.30,6.20))\n",
    "\n",
    "# Union de los dos dataframes de nuevo\n",
    "df_M = df_M_imput.append(df_M_reales)\n",
    "df_M.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Verificamos de nuevo cuales son los datos que no se encuetran dentro del mapa y los filtramos\n",
    "gdf_M = gpd.GeoDataFrame(df_M.copy(), geometry = gpd.points_from_xy(df_M.lon, df_M.lat), crs={'init': 'epsg:4326'})\n",
    "\n",
    "sectores = gpd.read_file('C:/Users/danie/Documents/Daniel/SIMAT/Límite_Catastral_de__Comunas_y_Corregimientos/Límite_Catastral_de__Comunas_y_Corregimientos.shp')\n",
    "\n",
    "ax = sectores.plot(color='white', edgecolor='black')\n",
    "gdf_M.plot(ax = ax, color = 'red')\n",
    "\n",
    "sectores_con_puntos_de_venta = gpd.sjoin(gdf_M, sectores, how = \"left\", op = 'intersects')\n",
    "sectores_con_puntos_de_venta_Medellin = sectores_con_puntos_de_venta.loc[sectores_con_puntos_de_venta.isna()['SECTOR'].apply(lambda value: not value)]\n",
    "\n",
    "ax = sectores.plot(color='white', edgecolor='black')\n",
    "sectores_con_puntos_de_venta_Medellin.plot(ax = ax, color = 'red')\n",
    "\n",
    "Medellin_Reales = np.zeros(df_M.shape[0])\n",
    "\n",
    "#print(sectores_con_puntos_de_venta_Medellin)\n",
    "for i in sectores_con_puntos_de_venta_Medellin.index:\n",
    "    Medellin_Reales[i] = 1\n",
    "df_M['Medellin for real'] = Medellin_Reales\n",
    "\n",
    "df_M = df_M.loc[df_M['Medellin for real'] == 1]\n",
    "\n",
    "#Eliminamos las columnas medellin y medellin reales ya que no son necesarias\n",
    "df_M = df_M.drop(['medellin', 'Medellin Reales', 'Medellin for real'], axis = 1)\n",
    "\n",
    "# Reorganizando el orden del dataset por preferencia propia\n",
    "df_M = df_M.reindex(columns = ['created_on', 'end_date', 'delta_time', 'lon', 'lat', 'bedrooms', 'bathrooms', 'surface_total', 'property_type', 'price'])\n",
    "\n",
    "#Crear nuevo dataset\n",
    "#df_M.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_cord_imput.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar tipos de vivienda (USANDO LIBRERIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "df_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_cord_imput.csv')\n",
    "df_M = df_M.loc[:, ~df_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Se crea un LE para sacar el nombre de cada categoria creada por el OHE\n",
    "le = LabelEncoder()\n",
    "df_M['property_type'] = le.fit_transform(df_M['property_type'])\n",
    "\n",
    "# Creacion del OHE, \"categories = 'auto'\" \n",
    "ohe = OneHotEncoder(categories = 'auto')\n",
    "\n",
    "# Se transforman las columnas\n",
    "out = ohe.fit_transform(df_M['property_type'].values[:,None])\n",
    "\n",
    "# Se convierten en un arreglo para poder trabajar con ella\n",
    "out_array = out.toarray()\n",
    "\n",
    "# Se crea un arreglo donde guardaremos las categorias de cada columna\n",
    "ohe_cols = []\n",
    "\n",
    "# Se crea un for en el que se crearan las columnas adicionales del df con sus respectivos nombres\n",
    "for category in ohe.categories_[0]:\n",
    "    col_add = str(le.classes_[category])\n",
    "    ohe_cols.append(col_add)\n",
    "    df_M[col_add] = out_array[:,int(category)]\n",
    "\n",
    "# Eliminamos columna de propiedad\n",
    "df_M = df_M.drop(['property_type'],axis = 1)\n",
    "\n",
    "# Guardamos el dataset\n",
    "# df_M.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_cord_imput_cat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar tipos de vivienda (MANERA MANUAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_sinnorm_sinimput_medellin_only.csv')\n",
    "df_M = df_M.loc[:, ~df_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#crear nuevo dataframe con estrutcura deseada\n",
    "# NO ES NECESARIO, MANERA MAS SIMPLE EN LA IMPUTACION DE COORDENADAS\n",
    "df_M_nuevo_completo = pd.DataFrame ({\n",
    "    'created_on' : df_M['created_on'],\n",
    "    'end_date' : df_M['end_date'],\n",
    "    'delta_time' : df_M['delta_time'],\n",
    "    'lon' : df_M['lon'],\n",
    "    'lat' : df_M['lat'],\n",
    "    'surface' : df_M['surface_total'],\n",
    "    'bedrooms' : df_M['bedrooms'],\n",
    "    'bathrooms' : df_M['bathrooms'],\n",
    "    'casa' : np.zeros(df_M.shape[0]),\n",
    "    'apartamento' : np.zeros(df_M.shape[0]),\n",
    "    'finca' : np.zeros(df_M.shape[0]),\n",
    "    'lote' : np.zeros(df_M.shape[0]),\n",
    "    'oficina' : np.zeros(df_M.shape[0]),\n",
    "    'local_comercial' : np.zeros(df_M.shape[0]),\n",
    "    'parqueadero' : np.zeros(df_M.shape[0]),\n",
    "    'deposito' : np.zeros(df_M.shape[0]),\n",
    "    'price' : df_M['price']\n",
    "})\n",
    "\n",
    "#extraer valores unicos en columna de propiedad\n",
    "unicos = df_M['property_type'].unique()\n",
    "\n",
    "#Categorizacion\n",
    "df_M_nuevo_completo['casa'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[0]) else 0)\n",
    "df_M_nuevo_completo['apartamento'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[2]) else 0)\n",
    "df_M_nuevo_completo['finca'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[6]) else 0)\n",
    "df_M_nuevo_completo['lote'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[5]) else 0)\n",
    "df_M_nuevo_completo['oficina'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[4]) else 0)\n",
    "df_M_nuevo_completo['local_comercial'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[8]) else 0)\n",
    "df_M_nuevo_completo['parqueadero'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[3]) else 0)\n",
    "df_M_nuevo_completo['deposito'] = df_M['property_type'].apply(lambda propiedad: 1 if (propiedad == unicos[7]) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputacion de Datos (BAÑOS Y CUARTOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24879, 18)\n",
      "(24596, 18)\n",
      "(23812, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_cord_imput_cat.csv')\n",
    "df_M = df_M.loc[:, ~df_M.columns.str.contains('^Unnamed')]\n",
    "print(df_M.shape)\n",
    "\n",
    "# Instaciar el imputador y aplicarlo al dataframe para las categorias de cuartos y baños\n",
    "imp_lon = SimpleImputer(strategy = 'mean')\n",
    "df_M['bedrooms'] = imp_lon.fit_transform(df_M['bedrooms'].values[:,None])\n",
    "\n",
    "imp_lat = SimpleImputer(strategy = 'mean')\n",
    "df_M['bathrooms'] = imp_lat.fit_transform(df_M['bathrooms'].values[:,None])\n",
    "\n",
    "# Eliminar Outliers\n",
    "df_M = df_M[df_M['bedrooms'] < df_M['bedrooms'].quantile(0.99)]\n",
    "print(df_M.shape)\n",
    "df_M = df_M[df_M['bathrooms'] < df_M['bathrooms'].quantile(0.99)]\n",
    "print(df_M.shape)\n",
    "\n",
    "# Guardamos el dataset\n",
    "# df_M.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_sinnorm_medellin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro de datos por comuna o corregimiento (USANDO LIBRERIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "df_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_sinnormt_medellin.csv')\n",
    "df_M = df_M.loc[:, ~df_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#Creacion del GeoDataFrame\n",
    "gdf_M = gpd.GeoDataFrame(df_M, geometry = gpd.points_from_xy(df_M.lon, df_M.lat), crs={'init': 'epsg:4326'})\n",
    "\n",
    "sectores = gpd.read_file('C:/Users/danie/Documents/Daniel/SIMAT/Límite_Catastral_de__Comunas_y_Corregimientos/Límite_Catastral_de__Comunas_y_Corregimientos.shp')\n",
    "\n",
    "#SpatialJoin\n",
    "sectores_con_puntos = gpd.sjoin(gdf_M, sectores, how = \"left\", op = 'intersects')\n",
    "\n",
    "# Se crea un LE para sacar el nombre de cada categoria creada por el OHE\n",
    "le = LabelEncoder()\n",
    "sectores_con_puntos['NOMBRE'] = le.fit_transform(sectores_con_puntos['NOMBRE'])\n",
    "\n",
    "# Creacion del OHE, \"categories = 'auto'\" \n",
    "ohe = OneHotEncoder(categories = 'auto')\n",
    "\n",
    "# Se transforman las columnas\n",
    "out = ohe.fit_transform(sectores_con_puntos['NOMBRE'].values[:,None])\n",
    "\n",
    "# Se convierten en un arreglo para poder trabajar con ella\n",
    "out_array = out.toarray()\n",
    "\n",
    "# Se crea un arreglo donde guardaremos las categorias de cada columna\n",
    "ohe_cols = []\n",
    "\n",
    "# Se crea un for en el que se crearan las columnas adicionales del df con sus respectivos nombres\n",
    "for category in ohe.categories_[0]:\n",
    "    col_add = str(le.classes_[category])\n",
    "    ohe_cols.append(col_add)\n",
    "    sectores_con_puntos[col_add] = out_array[:,int(category)]\n",
    "\n",
    "# Eliminamos columna de propiedad\n",
    "sectores_con_puntos = sectores_con_puntos.drop(['NOMBRE'],axis = 1)\n",
    "sectores_con_puntos = sectores_con_puntos.drop(['geometry', 'index_right', 'OBJECTID', 'COMUNA', 'SECTOR', 'SHAPEAREA', 'SHAPELEN'], axis = 1)\n",
    "\n",
    "# Guardamos el dataset\n",
    "# sectores_con_puntos.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_comuna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear columnas adicionales en dataframe (MANUAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comunas = sectores_con_puntos_de_venta['NOMBRE'].unique()\n",
    "\n",
    "df_M_nuevo_completo['laureles'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[0]) else 0)\n",
    "df_M_nuevo_completo['la_candelaria'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[1]) else 0)\n",
    "df_M_nuevo_completo['el_poblado'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[2]) else 0)\n",
    "df_M_nuevo_completo['villa_hermosa'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[3]) else 0)\n",
    "df_M_nuevo_completo['guayabal'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[4]) else 0)\n",
    "df_M_nuevo_completo['robledo'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[5]) else 0)\n",
    "df_M_nuevo_completo['altavista'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[6]) else 0)\n",
    "df_M_nuevo_completo['la_america'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[7]) else 0)\n",
    "df_M_nuevo_completo['buenos_aires'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[8]) else 0)\n",
    "df_M_nuevo_completo['belen'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[9]) else 0)\n",
    "df_M_nuevo_completo['aranjuez'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[10]) else 0)\n",
    "df_M_nuevo_completo['castilla'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[11]) else 0)\n",
    "df_M_nuevo_completo['santa_elena'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[12]) else 0)\n",
    "df_M_nuevo_completo['manrique'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[13]) else 0)\n",
    "df_M_nuevo_completo['san_javier'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[14]) else 0)\n",
    "df_M_nuevo_completo['popular'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[15]) else 0)\n",
    "df_M_nuevo_completo['prado'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[16]) else 0)\n",
    "df_M_nuevo_completo['12_octubre'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[17]) else 0)\n",
    "df_M_nuevo_completo['palmitas'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[18]) else 0)\n",
    "df_M_nuevo_completo['santa_cruz'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[19]) else 0)\n",
    "df_M_nuevo_completo['san_cristobal'] = sectores_con_puntos_de_venta['NOMBRE'].apply(lambda x: 1 if (x == comunas[20]) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis para determinar que variables escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_comuna.csv')\n",
    "df_M = df_M.loc[:, ~df_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Escalamiento de variables\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df_M[['delta_time', 'bedrooms', 'bathrooms', 'surface_total']])\n",
    "\n",
    "# El scaler nos da como resultado un numpy array asi que nosotros nos encargaremos de coveritirlo en un dataframe\n",
    "df_M_scaled = pd.DataFrame(scaled, columns = ['delta_time', 'bedrooms', 'bathrooms', 'surface_total'])\n",
    "\n",
    "# Cambiar columnas viejas por las escaladas\n",
    "df_M['delta_time'] = df_M_scaled['delta_time']\n",
    "df_M['bedrooms'] = df_M_scaled['bedrooms']\n",
    "df_M['bathrooms'] = df_M_scaled['bathrooms']\n",
    "df_M['surface_total'] = df_M_scaled['surface_total']\n",
    "df1 = df_M.loc[:,:'price']\n",
    "df1['price_real'] = df_M['price'].apply(lambda x: math.exp(x))\n",
    "df2 = df_M.loc[:,'Apartamento':]\n",
    "df_M = pd.concat([df1, df2], axis = 1)\n",
    "\n",
    "# Guardamos el dataset\n",
    "# df_M.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datosstand_con_comuna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler \n",
    "# no tiene ninguna diferencia con el StandardScaler en los resultados\n",
    "# se decide utilizar el previo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_M = pd.read_csv('C:/Users/danie/Documents/Daniel/SIMAT/datos_con_comuna.csv')\n",
    "df_M = df_M.loc[:, ~df_M.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Escalamiento de variables\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_M[['delta_time', 'bedrooms', 'bathrooms', 'surface_total']])\n",
    "\n",
    "# El scaler nos da como resultado un numpy array asi que nosotros nos encargaremos de coveritirlo en un dataframe\n",
    "df_M_scaled = pd.DataFrame(scaled, columns = ['delta_time', 'bedrooms', 'bathrooms', 'surface_total'])\n",
    "\n",
    "# Cambiar columnas viejas por las escaladas\n",
    "df_M['delta_time'] = df_M_scaled['delta_time']\n",
    "df_M['bedrooms'] = df_M_scaled['bedrooms']\n",
    "df_M['bathrooms'] = df_M_scaled['bathrooms']\n",
    "df_M['surface_total'] = df_M_scaled['surface_total']\n",
    "df1 = df_M.loc[:,:'price']\n",
    "df1['price_real'] = df_M['price'].apply(lambda x: math.exp(x))\n",
    "df2 = df_M.loc[:,'Apartamento':]\n",
    "df_M = pd.concat([df1, df2], axis = 1)\n",
    "\n",
    "# Guardamos el dataset\n",
    "# df_M.to_csv('C:/Users/danie/Documents/Daniel/SIMAT/datosminmax_con_comuna.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
